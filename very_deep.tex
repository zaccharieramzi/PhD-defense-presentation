\section{Going even deeper}
\subsection{Implicit models}

\begin{frame}{Why should we go deep?}
    % with deeper models, comes better performance
    % figure of Pezzotti et al.
    The general empirical gist of DL is that with deeper models comes better performance.
    \begin{figure}
        \begin{overprint}
            \onslide<2>\centering\includegraphics[width=0.4\textwidth]{Figures/shine_figures/more_layers.jpg}\caption{Credits: \href{reddit.com/r/ProgrammerHumor/comments/5si1f0/machine_learning_approaches/}{reddit.com/r/ProgrammerHumor/comments/5si1f0/machine\_learning\_approaches/}}
            \onslide<3>\centering\includegraphics[width=0.78\textwidth]{Figures/shine_figures/pezzotti.png}\caption{\textbf{Performance of an unrolled MRI reconstruction network function of the number of iteration units (blocks).}\footfullcite{Pezzotti2020AnReconstruction}}
        \end{overprint}
    \end{figure}
\end{frame}

\begin{frame}{Can we go deeper?}
    % in the current state not: activations + constrained memory
    The big problem at hand is that deeper models need more memory to train, and the memory we have is constrained.
    \pause

    This memory requirement comes primarily from the activations of the model, not from the model weights.
    \pause

    \begin{center}
        \begin{tikzpicture}[
            font=\Large, node distance=5em,>=stealth,
            highlight/.style={rounded rectangle, draw=red!67, dashed, inner sep=0.1em},
        ]
            % nodes
            \node (input) {$\ldots$};
            \node (op) [rounded rectangle, right=of input, draw=black!87] {$\exp^{\thetab^\top \cdot}$};
            \node (output) [right=of op] {$\ldots$};
            \node (op_params) [below=of op] {$\thetab$};
            % arrows
            %% input to op
            \draw[->] (input.east) -- node [midway,below] {$\xb$} node [midway,above, visible on=<5->] {
                $\frac{\partial \mathcal{L}}{\partial \xb}$\alt<6->{\hspace{-0.5em}
                    \alt<7->{
                        $= \frac{\partial \mathcal{L}}{\partial \yb} \tikzmarknode[highlight]{y_x}{\yb} . \thetab $
                    }{
                        $= \frac{\partial \mathcal{L}}{\partial \yb} \frac{\partial \yb}{\partial \xb}$
                    }
                }{ ?}
            } (op.west);
            %% op to output
            \draw[->] (op) -- node (y) [midway,below] {$\alt<7->{\tikzmarknode[highlight]{y}{\yb}}{\yb}$} node [midway,above, visible on=<4->] {$\hookleftarrow \frac{\partial \mathcal{L}}{\partial \yb}$}  (output);
            %% op to op_params
            \draw[line width=0.1em] (op.south) -- node [midway, right, visible on=<5->] {
                $\frac{\partial \mathcal{L}}{\partial \thetab}$ \alt<6->{\hspace{-0.5em}
                    \alt<7->{
                        $= \frac{\partial \mathcal{L}}{\partial \yb} \tikzmarknode[highlight]{y_theta}{\yb} . \xb $
                    }{
                        $= \frac{\partial \mathcal{L}}{\partial \yb} \frac{\partial \yb}{\partial \thetab}$
                    }
                }{?}
            } (op_params.north);
        \end{tikzpicture}    
    \end{center}
    
\end{frame}

\begin{frame}{The modeling solutions}
    % gradient checkpointing
    % Invertible networks
    % Implicit models
    On the modeling side, several solutions exist:
    \begin{itemize}[<+->]
        \item Gradient checkpointing~\citep{Chen2016TrainingCost}
        \item Invertible networks~\citep{Gomez2017TheActivations,Sander2021MomentumNetworks}
        \item Implicit models~\citep{Chen2018NeuralEquations,Bai2019DeepModels}
    \end{itemize}
\end{frame}

\begin{frame}{Deep Equilibrium networks - 1}
    % give equation and how to compute the gradient with IFT
    \citet{Bai2019DeepModels} introduced Deep Equilibrium networks~(DEQs), a type of implicit model.
    The output of DEQs is defined implicitly as the solution to a fixed-point equation.

    \begin{equation*}
        h_{\thetab}(\xb) = \zb^\star, \text{ where } \zb^\star = f_\thetab(\zb^\star, \xb)
    \end{equation*}

    \pause

    In practice, we solve this equation with a quasi-Newton method.
\end{frame}

\begin{frame}{Deep Equilibrium networks - 2}
    The big question is now: how do I compute the gradient $\frac{\partial \mathcal{L}}{\partial \thetab}$?
    \pause

    The \textbf{Implicit Function Theorem} gives us just that:
    \begin{theorem}[Hypergradient~\citep{Krantz2013TheApplications,Bai2019DeepModels}]
        Let $\thetab \in \mathbb{R}^p$ be a set of parameters, let $\mathcal{L}: \mathbb{R}^d \rightarrow \mathbb{R}$ be a loss function and $g_{\thetab}: \mathbb{R}^d \rightarrow \mathbb{R}^d$ be a root-defining function.
Let $\zb^\star \in  \mathbb{R}^d$ such that $g_{\thetab}(\zb^\star) = 0$ and $J_{g_{\thetab}}(\zb^\star) = \frac{\partial g_{\theta}}{\partial \zb}\Bigr|_{\zb^\star}$ is invertible, then the gradient of the loss $\mathcal{L}$ wrt. $\thetab$, called Hypergradient, is given by
\begin{equation*}
    \frac{\partial \mathcal{L}}{\partial \thetab}\Bigr|_{\zb^\star} = \nabla_\zb \mathcal{L}(\zb^\star)^\top J_{g_{\thetab}}(\zb^\star)^{-1} \frac{\partial g_{\thetab}}{\partial \thetab}\Bigr|_{\zb^\star} \, .
\end{equation*}
    \end{theorem}
    \pause

    Key for our memory problem: it does not rely on any activations you could have when solving the fixed-point equation.
\end{frame}

\subsection{SHINE}
\begin{frame}{The limits of DEQs}
    % they are slow to train
    DEQs achieve excellent results in NLP~(Natural Language Processing) and CV~(Computer Vision) problems, but they are slow to train.

    \begin{figure}
        \centering
        \includegraphics[width=0.6\textwidth]{Figures/shine_figures/deq_memory.png}
        \caption{\textbf{Performance, memory and training speed of DEQs.}~\citep{Bai2020MultiscaleModels}}
    \end{figure}
\end{frame}

\begin{frame}{Why are DEQs slow?}
    % bc of Jacobian inversion
    If we look back the equation used to compute the gradient of DEQs:
    \begin{equation*}
        \frac{\partial \mathcal{L}}{\partial \thetab}\Bigr|_{\zb^\star} = \nabla_\zb \mathcal{L}(\zb^\star)^\top J_{g_{\thetab}}(\zb^\star)^{-1} \frac{\partial g_{\thetab}}{\partial \thetab}\Bigr|_{\zb^\star} \, ,
    \end{equation*}
    we see that we need to invert a huge matrix $J_{g_{\thetab}}(\zb^\star)^{-1}$.
\end{frame}

\begin{frame}{Can we avoid the Jacobian inversion?}
    % yes: reuse a by-product of the forward pass, share the inverse estimate
\end{frame}

\begin{frame}{Application to Hyperparameter optimization}
    % it's also a bilevel prob
\end{frame}

\begin{frame}{Results on DEQs}

\end{frame}