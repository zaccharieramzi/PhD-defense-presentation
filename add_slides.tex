% cSpell: disable
\appendix

\begin{frame}[noframenumbering, plain]{}
    Backup slides
\end{frame}

\begin{frame}{Importance of MRI - 1}
    % how many MRI scans / scanners
    % how likely is it that you will get an MRI in your life
    % Based on a rough extrapolation of the following figure, there is a 99.9\% chance that you will get an MRI in your life in France.
    99.9\% chance you will get an MRI.
    \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{Figures/intro_figures/num_mri_scans.png}
        \caption{\label{fig:num-mri-scans} \textbf{Number of MRI scans per year per 1000 population}: figure courtesy of \citet{OECDMRI}.}
    \end{figure}
\end{frame}

% There is a reason for that: MRI can help diagnose many different conditions
\begin{frame}{Importance of MRI - 2}
    % list of conditions
    % This illustration provides a non-exhaustive list of all the diagnoses that can be carried out with MRI.
    \begin{figure}
        \centering
        \includegraphics[height=0.6\textheight]{Figures/intro_figures/What_can_we_diagnose_with_MRI.pdf}
        \caption{\label{fig:diagnose-mri} \textbf{What can we diagnose with MRI?}  Info compiled from \citet{reimer2010clinical,runge2019essentials}.}
    \end{figure}
\end{frame}

\begin{frame}{Physics of MRI - 1}
    % Because all spins get excited, we get a global RF pulse that is the weighted sum of the contribution of all spins' relaxation RF pulses: global information
    % We can obtain "multiple global information", by changing a bit the magnetic field spatially using gradients
    % Signal equation
    % However, the FID carries global information.
    FID: global info.
    \pause

    % Using magnetic \textbf{gradients} enables changing a bit the magnetic field spatially, and therefore changing the global information depending on local factors.
    Magnetic \textbf{gradients} $\Rightarrow$ change the magnetic field spatially.
    \pause

    % This allows us to receive a temporal signal of the form:
    Temporal signal:
    \begin{equation*}
        \vspace{\baselineskip}
        \tikzmarknode{S}{\highlight{blue}{$S_{tr}(t)$}} \propto \omega_0  \int_{V_s} B_{tr} \tikzmarknode{M}{\highlight{green}{$M_{tr}(t, \rb)$}} e^{-\imath \gamma \rb \cdot \int_0^t \tikzmarknode{G}{\highlight{red}{$\Gb(\tau)$}}  \dif \tau} \dif \rb
    \end{equation*}
    \begin{tikzpicture}[overlay,remember picture,>=stealth,nodes={align=left,inner ysep=1pt},<-]
        % For "S"
        \onslide<4->{
        \path (S.south) ++ (0,-2em) node[anchor=south east,color=blue!67] (exp_S){\textbf{Recorded MR signal}};
        \draw [color=blue!87](S.south) |- ([xshift=-0.3ex,color=blue]exp_S.south west);
        }
        % For "M"
        \onslide<5->{
        \path (M.south) ++ (0, -6em) node[anchor=south east,color=DarkGreen!77,align=right] (exp_M){\textbf{Magnetic field in each location $\rb$,}\\ \textbf{$\propto$ spin density $\rho(\rb)$}};
        \draw [color=DarkGreen!87](M.south) |- ([xshift=-0.3ex,color=DarkGreen]exp_M.south west);
        }
        % For "G"
        \onslide<6->{
        \path (G.south) ++ (0, -2em) node[anchor=north west,color=DarkRed!47] (exp_G){\textbf{Temporal gradients,}\\\textbf{controlled by the operator}};
        \draw [color=DarkRed!87](G.south) |- ([xshift=-0.3ex,color=DarkRed]exp_G.south east);
        }
    \end{tikzpicture}
\end{frame}

% XXX write those
\begin{frame}{MR signal derivation}
    \only<1>{
        Bloch equations:
        \begin{equation*}
            \begin{split}
                \diff{\Mb_{tr}}{t} &= - \frac{ \tikzmarknode{mtr}{\Mb_{tr}} }{T_2} \\
            \diff{\Mb_l}{t} &= \frac{ \tikzmarknode{mo}{\Mb_0}  - \tikzmarknode{ml}{\Mb_l} }{T_1}
            \end{split}
        \end{equation*}

        \vspace{4em}
        \begin{tikzpicture}[overlay,remember picture,>=stealth,nodes={align=left,inner ysep=1pt},<-]
            % For "mtr"
            \path (mtr.north) ++ (0,2em) node[anchor=south west,color=black!87] (exp_mtr){
                \textbf{Transverse component of $\Mb$}};
            \draw [color=black!87](mtr.north) |- ([xshift=-0.3ex,color=black]exp_mtr.south east);
            % For "mo"
            \path (mo.south) ++ (0, -4em) node[anchor=south east,color=black!87] (exp_mo){
                \textbf{Equilibrium state}};
            \draw [color=black!87](mo.south) |- ([xshift=-0.3ex,color=black]exp_mo.south west);
            % For "ml"
            \path (ml.south) ++ (0,-4em) node[anchor=south west,color=black!87] (exp_ml){
                \textbf{Longitudinal component of $\Mb$}};
            \draw [color=black!87](ml.south) |- ([xshift=-0.3ex,color=black]exp_ml.south east);
        \end{tikzpicture}
    }
    \only<2>{
        Solution:
        \begin{equation*}
            \begin{split}
                \Mb_{tr}(t, \rb) &= \tikzmarknode{mtro}{\Mb_{tr}(0, \rb)} e^{-\frac{t}{T_2}}\\
                \Mb_l(t, \rb) &= \Mb_l(0, \rb) e^{-\frac{t}{T_1}} + \Mb_0 (1 - e^{-\frac{t}{T_1}})
            \end{split}
        \end{equation*}
        \vspace{4em}
        \begin{tikzpicture}[overlay,remember picture,>=stealth,nodes={align=left,inner ysep=1pt},<-]
            % For "mtro"
            \path (mtro.north) ++ (0, 2em) node[anchor=south west,color=black!87] (exp_mtro){
                $|\Mb_{tr}(0, \rb)| = \frac14 \rho(\rb) \frac{\gamma^2 \hbar^2}{kT}B_0$};
            \draw [color=black!87](mtro.north) |- ([xshift=-0.3ex,color=black]exp_mtro.south east);
        \end{tikzpicture}
    }
    \only<3>{
        EF force in antenna:
        \begin{equation*}
            S(t) = - \diff{}{t} \int_{V_s} \Bb_1 \cdot \Mb(t, \rb) \dif \rb
        \end{equation*}
    }

\end{frame}

\begin{frame}{Fourier Transform and MRI}
    \begin{multicols}{2}
        MR signal:
        \begin{equation*}
            \vspace{\baselineskip}
            S_{tr}(t) \propto \omega_0  \int_{V_s} B_{tr} M_{tr}(t, \rb) e^{-\imath \gamma \rb \cdot \int_0^t \Gb(\tau)  \dif \tau} \dif \rb
        \end{equation*}
        \newpage
        Fourier Transform of a signal:
        \begin{equation*}
            \hat{f}(\omega) = \int_{-\infty}^{\infty} f(x) e^{-2\pi \imath \omega x} dx
        \end{equation*}
    \end{multicols}
\end{frame}

\begin{frame}{The example of GRAPPA}
    \begin{figure}
        \centering
        \includegraphics[height=0.6\textheight]{Figures/intro_figures/GRAPPA.jpeg}
        \caption{\label{fig:GRAPPA}\textbf{GRAPPA illustration.} Image courtesy of \citet{deshmane2012parallel}.
        }
    \end{figure}
\end{frame}


\begin{frame}{Noise model for MRI}
    Bayesian view of MRI reconstruction:
    \begin{equation*}
        \argmax_{\xb} p(\xb|\ybb) \propto p(\ybb|\xb) p(\xb)
    \end{equation*}
    If we consider an additive white Gaussian noise model:
    \begin{equation*}
        p(\ybb|\xb) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2\sigma^2} \|\mathcal{A}\xb - \ybb \| ^2 }
    \end{equation*}
    We retrieve:
    \begin{equation*}
        - \log p(\ybb|\xb) \propto \|\mathcal{A}\xb - \ybb \| ^2 + \text{cst}
    \end{equation*}
\end{frame}

\begin{frame}{Sparsity and Inverse Problems}
    % explain the concept of sparsity and its link to recovery guarantees
    \begin{definition}[Sparsity]
        A vector $\xb \in \mathbb{C}^n$ is called $s$-sparse if it contains at most $s$ non-zero entries.
    \end{definition}

    \begin{lemma}[Optimization reformulation of sparse vector recovery~\citep{Foucart2013SparseSystems}]
        For a given sparsity $s$, and $s$-sparse vector $\xb$:
    \begin{enumerate}[(a)]
        \item The vector $\xb$ is the unique $s$-sparse solution of $\Ab \xb = \yb$, that is $\{\zb \in \mathbb{C}^n : \Ab \zb = \Ab \xb, \|z\|_0 \leq s\} = \{\xb\}$
        \item The vector $\xb$ can be reconstructed as the unique solution of:
        \begin{equation*}
            \label{eq:cs-problem-l0-min}
            \min_{\zb \in \mathbb{C}^n} \|\zb\|_0 \quad \text{subject to} \quad \Ab \zb = \yb
        \end{equation*}
    \end{enumerate}
    \end{lemma}
\end{frame}

\begin{frame}{Recovery guarantees}
    \begin{theorem}[{{\citep[Theorem~2.13]{Foucart2013SparseSystems}}}]
        The following properties are equivalent:
\begin{enumerate}[(a)]
    \item Every $s$-sparse vector $\xb \in \mathbb{C}^n$ is the unique $s$-sparse solution of $\Ab \zb = \Ab \xb$, that is, if $\Ab \xb = \Ab \zb$ and both $\xb$ and $\zb$ are $s$-sparse, then $\xb = \zb$.
    \item The null space $\ker(\Ab)$ does not contain any $2s$-sparse vector other than the zero.
    \item Every set of $2s$ columns of $\Ab$ is linearly independent.
\end{enumerate}
    \end{theorem}
\end{frame}

\begin{frame}{Proximity operator}
    Definition:
    \begin{equation*}
        \operatorname{prox}_{\mathcal{R}}(\xb) = \argmin_{\zb \in \mathcal{H}} \mathcal{R}(\zb) + \frac12\|\zb - \xb\|_2^2
    \end{equation*}
    2 intuitions:
    \begin{itemize}
        \item Prox. of indicator of $\mathcal{C}$, a convex set, is the projection onto $\mathcal{C}$.
        \item Prox. of a smooth function is its gradient step.
    \end{itemize}
\end{frame}

\begin{frame}{The power of Deep Learning}
    % we want to learn a complicated function that tells us whether an image is an MR image
    % similarly deep learning has been able to build functions that tell whether an image is that of a dog or a cat
    % universal approx
    % We want to learn a complicated function that tells us whether a complex-valued vector is an MR image or not.
    % We want a function that tells us whether a vector is an MR image or not.
    The prior is a complicated visual function.
    \pause

    Deep Learning~(DL) has been used to build complicated functions:
    {\Large
        \begin{equation*}
            \tikzmarknode{nn}{\highlight{brown}{$f_\thetab$}}\left( \adjincludegraphics[valign=c,width=0.2\textwidth]{Figures/intro_figures/chowchow.jpg} \right) = \text{"DOG"}
        \end{equation*}
        \begin{tikzpicture}[overlay,remember picture,>=stealth,nodes={align=left,inner ysep=1pt},<-,font=\normalsize]
            % For "nn"
            \onslide<3->{
            \path (nn.south) ++ (0, -4.5em) node[anchor=south west,color=brown!87] (exp_nn){
                Neural network:\\a chain of elementary linear \& nonlinear functions};
            \draw [color=brown!87](nn.south) |- ([xshift=-0.3ex,color=brown]exp_nn.south east);
            }
        \end{tikzpicture}
    }
\end{frame}

\begin{frame}{Formalism - 1}
    % supervised learning objective function
    % The classical framework for DL is supervised learning:
    Supervised learning:
    \vspace{\baselineskip}

    \begin{equation*}
        \argmin_{\tikzmarknode{params}{\highlight{orange}{$\thetab \in \Theta$}}} \tikzmarknode{sum}{\highlight{purple}{$\sum\limits_{(\xb_i, \yb_i) \in \mathcal{D}}$}} \tikzmarknode{loss}{\highlight{green}{$\mathcal{L}$}}(\tikzmarknode{nn}{\highlight{brown}{$f_{\thetab}$}} (\tikzmarknode{input}{\highlight{blue}{$\xb_i$}}), \tikzmarknode{label}{\highlight{red}{$\yb_i$}}, \thetab)
    \end{equation*}
    % XXX add visual aid to formalism
    \begin{tikzpicture}[overlay,remember picture,>=stealth,nodes={align=left,inner ysep=1pt},<-]
        % For "input"
        \onslide<2->{
        \path (input.north) ++ (0,1.5em) node[anchor=south west,color=blue!87] (exp_input){
            \includegraphics[width=0.05\textwidth]{Figures/intro_figures/chowchow.jpg} input};
        \draw [color=blue!87](input.north) |- ([xshift=-0.3ex,color=blue]exp_input.south east);
        }
        % For "label"
        \onslide<3->{
        \path (label.south) ++ (0, -2.5em) node[anchor=south west,color=red!87] (exp_label){
            "DOG", label};
        \draw [color=red!87](label.south) |- ([xshift=-0.3ex,color=red]exp_label.south east);
        }
        % For "nn"
        \onslide<4->{
        \path (nn.south) ++ (0, -4.5em) node[anchor=south east,color=brown!87] (exp_nn){
            neural network};
        \draw [color=brown!87](nn.south) |- ([xshift=-0.3ex,color=brown]exp_nn.south west);
        }
        % For "loss"
        \onslide<5->{
        \path (loss.north) ++ (0, 1.5em) node[anchor=south west,color=DarkGreen!77] (exp_loss){
            loss};
        \draw [color=DarkGreen!87](loss.north) |- ([xshift=-0.3ex,color=DarkGreen]exp_loss.south east);
        }
        % For "sum"
        \onslide<6->{
        \path (sum.south) ++ (0, -2em) node[anchor=south east,color=purple!87] (exp_sum){
            Estimator of the expected value};
        \draw [color=purple!87](sum.south) |- ([xshift=-0.3ex,color=purple]exp_sum.south west);
        }

        % For "params"
        \onslide<7->{
        \path (params.west) ++ (-2em, 0) node[anchor=east,color=orange!87] (exp_params){
            Parameters};
        \draw [color=orange!87](params.west) -- ([xshift=-0.3ex,color=orange]exp_params.east) -- ([xshift=-0.3ex,color=orange]exp_params.south east) -- ([xshift=-0.3ex,color=orange]exp_params.south west);
        }

    \end{tikzpicture}
\end{frame}

\begin{frame}{Formalism - 2}
    % Stochastic Gradient descent and chain rule
    To solve the previous equation we will use two main tools:

    \begin{enumerate}
        \item \alt<2>{Stochastic Gradient Descent~(SGD)}{\highlight{blue}{Stochastic Gradient Descent~(SGD)}};
        \item<2> \highlight{blue}{Chain rule}.
    \end{enumerate}


        \begin{block}{Definition}
            \only<1>{An algorithm to solve the previous optimization problem based on first order derivatives.}
            \only<2>{
                A property allowing us to compute easily derivatives of compound functions.
                \begin{equation*}
                    \frac{\partial f}{\partial y} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial y}
                \end{equation*}
            }
        \end{block}

\end{frame}

\begin{frame}{Requirements for Deep Learning}
    % Great that I can do that, but does it take ?
    % Data, compute + memory, framework
    % accept that it's "black-box"
    What does it take to use DL in a problem?
    \begin{itemize}[<+->]
        \item data
        \item compute \& memory
        \item development framework
        \item accepting that it's "black-box"
    \end{itemize}
\end{frame}


\begin{frame}{CNN}
    Convolutional Neural Network~(CNN): chain of Convolution + Nonlinearity.
\end{frame}

\begin{frame}{U-net}
    \begin{figure}
        \centering
        \includegraphics[height=0.6\textheight]{Figures/add_slides/unet_hires.pdf}
        \caption{Illustration from the original paper.\footfullcite{ronneberger2015u}}
    \end{figure}
\end{frame}

\begin{frame}{MWCNN}
    \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{Figures/add_slides/mwcnn.pdf}
        \caption{Illustration from the original paper.\footfullcite{Liu2018}}
    \end{figure}
\end{frame}

\begin{frame}{fastMRI dataset}
    \href{https://fastmri.org/}{fastmri.org}
    \begin{multicols}{2}
        \textbf{Knee}
        \begin{itemize}
            \item 973 train volumes, 199 validation volumes
            \item 2 contrasts: PD and PDFS
            \item 15 coils, 1.5T/3T, 320 x 320, 0.5 mm x 0.5 mm, Cartesian 2D TSE
        \end{itemize}
        \newpage
        \textbf{Brain}
        \begin{itemize}
            \item 4469 train volumes, 1378 validation volumes
            \item 4 contrasts: T1, T1 post injection, T2, FLAIR
            \item different locations, different coil architecture
            \item 1.5T/3T, 320 x 320 (with exceptions), 0.5 mm x 0.5 mm, Cartesian 2D TSE
        \end{itemize}
    \end{multicols}
\end{frame}

\begin{frame}{2020 fastMRI challenge}
    \begin{itemize}
        \item 2nd edition
        \item 8 teams
        \item Brain data
        \item 3 tracks: 4X, 8X, transfer
    \end{itemize}
\end{frame}

\begin{frame}{What did the winners do that I did not}
    \begin{itemize}
        \item Unclear feature multi-domain learning
        \item 3D Post-processing (main network is 2D)
        \item Distributed training (4 GPUs)
    \end{itemize}
\end{frame}

\begin{frame}{autoMAP}
    \begin{figure}
        \centering
        \includegraphics[width=0.67\textwidth]{Figures/add_slides/automap.png}
        \caption{Illustration from the original paper.\footfullcite{Zhu2018}}
    \end{figure}
\end{frame}

\begin{frame}{XPDNet - ct'ed}
    \begin{center}
        \scalebox{0.85}{
        \begin{algorithm}[H]
            \SetAlgoLined
            \KwData{$\ybb$ the k-space data, $\Omega$ the Cartesian trajectory, $\Sbb$ the coarse estimates of the sensitivity maps}
            \KwResult{$\xb$, the reconstructed magnitude MR image}
             $\Sbb = g_{\theta_r}(\Sbb)$ ; \tcp{Sensitivity maps refinement}
             Update ${\cal A}$ and ${\cal A}^H$\;
             $\xb =  {\cal A}^H \ybb$\;

             $\xb_b = [\xb, \xb, \xb, \xb, \xb]$; \tcp{Buffer creation, in practice concatenation along the channel dimension}
             \For{$i \leftarrow 1$ \KwTo $N_C$}{
               $\ybb_{res} = {\cal A}\, \xb_b[0] - \ybb $; \tcp{Data consistency}
              $\xb_{dc} = {\cal A}^H \ybb_{res}$; \tcp{Density compensation}
              $\xb_b = \xb_b + f_{\theta_i}([\xb_b, \xb_{dc}]))$; \tcp{Proximity operator learning and nonlinear acceleration scheme}
             }
             $\xb = |\xb_b[0]|$ \tcp{Magnitude computation}
             \caption{\emph{XPDNet}.}
        \end{algorithm}
    }
    \end{center}
\end{frame}

\begin{frame}{PDNet \& Recurrent Inference Machines~(RIMs)}
    Bayesian Inverse Problem formulation:
    \begin{equation*}
        \argmax_x p(x|y) \propto p(y|x) p(x)
    \end{equation*}
    Updates:
    \begin{equation*}
        x_{n+1} = x_n + \epsilon_n \nabla_x (\log p(y|x) + \log p(x))(x_n)
    \end{equation*}
    RIMs generalize to:\footfullcite{Putzky2017RecurrentProblems}
    \begin{equation*}
        x_{n+1} = x_n + g(\nabla_x(\log p(y|x))(x_n), x_n)
    \end{equation*}
\end{frame}

\begin{frame}{NC-PDNet - ct'ed}
    \begin{center}
        \scalebox{0.85}{
        \begin{algorithm}[H]
            \SetAlgoLined
            \KwData{$\ybb$ the k-space data, $\Omega$ the non-Cartesian trajectories, $\db$ the pre-computed DCp weights, $\Sbb$ the coarse estimates of the sensitivity maps}
            \KwResult{$\xb$, the reconstructed magnitude MR image}
             $\Sbb = g_{\theta_r}(\Sbb)$ ; \tcp{Sensitivity maps refinement}
             Update ${\cal A}$ and ${\cal A}^H$\;
             $\xb =  {\cal A}^H \ybb$\;

             $\xb_b = [\xb, \xb, \xb, \xb, \xb]$; \tcp{Buffer creation, in practice concatenation along the channel dimension}
             \For{$i \leftarrow 1$ \KwTo $N_C$}{
               $\ybb_{res} = {\cal A}\, \xb_b[0] - \ybb $; \tcp{Data consistency}
              $\xb_{dc} = {\cal A}^H \db\, \ybb_{res}$; \tcp{Density compensation}
              $\xb_b = \xb_b + f_{\theta_i}([\xb_b, \xb_{dc}]))$; \tcp{Proximity operator learning and nonlinear acceleration scheme}
             }
             $\xb = |\xb_b[0]|$ \tcp{Magnitude computation}
             \caption{\emph{NC-PDNet}: Density compensated Primal Dual unrolled neural network over $N_C$ iterations.}
             \label{alg:NC-PDNet}
        \end{algorithm}
    }
    \end{center}
\end{frame}

\begin{frame}{Density Compensation}
    \begin{equation*}
            \db_{n+1} = \frac{\db_n}{{\cal F}_{\Omega} {\cal F}^H_{\Omega} \db_n}
    \end{equation*}
\end{frame}

\begin{frame}{Image quality metrics}
    \begin{multicols}{2}
        \textbf{Peak Signal to Noise Ratio~(PSNR)}:
        \begin{equation*}
            \operatorname{PSNR}(\xb, \hat{\xb}) = 10 \log_{10}\left( \frac{\max_i{\xb_i}}{\frac1n \|\xb - \hat{\xb}\|^2_2}\right)
        \end{equation*}
        \newpage
        \textbf{Structural Similarity Index~(SSIM)}:
        \begin{equation*}
            \operatorname{SSIM}(\xb, \hat{\xb}) = [l(\xb, \hat{\xb})]^\alpha . [c(\xb, \hat{\xb})]^\beta . [s(\xb, \hat{\xb})]^\gamma
        \end{equation*}
        \begin{itemize}
            \item $l$: luminance
            \item $c$: contrast
            \item $s$: structure
        \end{itemize}
    \end{multicols}
    Problem: not a good correlation with the actual visual quality.
\end{frame}

\begin{frame}{quasi-Newton methods - 1}
    Idea: replace the costly Jacobian inverse with a qN matrix $\Bb^{-1}$.\\
    \begin{equation*}
        f(x^\star) = 0
    \end{equation*}
    \begin{multicols}{2}

            \begin{center}
                \textbf{Newton Methods}
                \begin{equation*}
                    x_{n+1} = x_n -\frac{\partial f}{\partial x}(x_n)^{-1}f(x_n)
                \end{equation*}
            \end{center}
            \newpage
            \begin{center}
                \textbf{Quasi-Newton Methods}
                \begin{equation*}
                    x_{n+1} = x_n -\Bb_n^{-1}f(x_n)
                \end{equation*}
            \end{center}
            Update $\Bb_n$ and its inverse with the Sherma-Morrison formula.
    \end{multicols}
\end{frame}

\begin{frame}{quasi-Newton methods - 2}
    Secant conditions: set of conditions $\Bb$ must verify.\\
    Typically: $\Bb_n(x_n - x_{n-1}) = f(x_n) - f(x_{n-1})$.\\
    Multiple solutions $\Rightarrow$ regularization with $\Bb_n = \argmin_{\Bb: \Bb \Delta x_n = \Delta f_n} \|\Bb - \Bb_{n-1}\|$
\end{frame}

\begin{frame}{OPA - 1}
    Outer Problem Awareness: modify the inner problem resolution depending on the outer problem.\\

    Additional updates of $\Bb$ with the OPA direction: $\eb_n = t_n \Bb_n^{-1} \frac{\partial g_{\theta}}{\partial \theta}\Bigr|_{\zb_n}$.
\end{frame}

\begin{frame}{OPA - 2}
    \begin{center}
        \scalebox{.5}{
    \begin{algorithm}[H]
        \SetAlgoRefName{LBFGS}
        \DontPrintSemicolon
        \caption{(Limited memory) BFGS method with OPA.}
        \label{alg:LBFGS}
        \KwIn{ initial guess $(\zb_0, \Bb_0^{-1})$, where $\Bb_0^{-1}$ is symmetric and positive definite, tolerance $\epsilon>0$, frequency of additional updates $M\in\mathbb{N}$, memory limit $L\in\mathbb{N}\cup\{\infty\}$, $(t_n)$ a null sequence of positive numbers with $\sum_n t_n<\infty$	}
        Let $F := \nabla_\zb g_{\theta}$\\
        \For{ $n=0,1,2,\ldots$ }
        {
            \lIf{$\norm{F(\zb_n)}\leq\epsilon$ }{let $\zb^\star:=\zb_n$ and let $\Bb:=\Bb_n$; STOP}
            Let $\hat \Bb_n^{-1}:=\Bb_n^{-1}$\\
            \If{$(n\operatorname{mod} M)=0$}{let $\eb_n:=t_n \Bb_n^{-1} \frac{\partial g_{\theta}}{\partial \theta}\Bigr|_{\zb_n}$,
                $\hat \yb_n:=F(\zb_n+\eb_n)-F(\zb_n)$ and $\hat \rb_n:=(\eb_n)^\top \hat \yb_n$\\
            \If{$\hat \rb_n>0$}{let $\hat \ab_n:=\eb_n - \Bb_n^{-1} \hat \yb_n$ and let
            \begin{equation*}
                \hat \Bb_n^{-1} := \Bb_n^{-1} + \frac{\hat \ab_n (\eb_n)^\top + \eb_n (\hat \ab_n)^\top}{\hat \rb_n} -
                \frac{(\hat \ab_n)^\top \hat \yb_n}{(\hat \rb_n)^2} \eb_n (\eb_n)^\top
            \end{equation*}
            }
        }
            Let $\Bb_n^{-1}:=\hat \Bb_n^{-1}$\\
            \lIf{$n\geq L$}{remove update $n-L$ from $\Bb_n^{-1}$}
            Let $p_n := -\Bb_n^{-1} F(\zb_n)$\\
            Obtain $\alpha_n$ via line-search and let $\sb_n := \alpha_n p_n$\\
            Let $\zb_{n+1}:=\zb_n+\sb_n$, $\yb_n:=F(\zb_{n+1})-F(\zb_n)$ and $\rb_n:=(\sb_n)^\top \yb_n$\\
            \uIf{$\rb_n>0$}{let $\ab_n:=\sb_n-\Bb_n^{-1} \yb_n$ and let
            \begin{equation*}
                \Bb_{n+1}^{-1} := \Bb_n^{-1} + \frac{\ab_n (\sb_n)^\top + \sb_n (\ab_n)^\top}{\rb_n} -
                \frac{(\ab_n)^\top \yb_n}{(\rb_n)^2} \sb_n (\sb_n)^\top
            \end{equation*}
            }
            \lElse{let $\Bb_{n+1}^{-1}:=\Bb_n^{-1}$}
            \lIf{$n\geq L$}{remove update $n-L$ from $\Bb_{n+1}^{-1}$}
        }
        \KwOut{$\zb^\star$, $\Bb$}
    \end{algorithm}
    }
    \end{center}
\end{frame}

\begin{frame}{Adjoint Broyden}
    For DEQs, vanilla OPA direction involves $\frac{\partial g_{\thetab}}{\partial \thetab}\Bigr|_{\zb_n}$ $\Rightarrow$ inefficient.\\

    Recall that the SHINE direction is: $\pb_\thetab =  \highlight{blue}{$\nabla_\zb \mathcal{L}(\zb^\star)$} B^{-1} \frac{\partial g_{\thetab}}{\partial \thetab}\Bigr|_{\zb^\star}$.\\
    Other direction, \highlight{blue}{$\nabla_\zb \mathcal{L}(\zb^\star)$}, in left-multiplication $\Rightarrow$ Adjoint Broyden for the secant condition to work.
\end{frame}

\begin{frame}{Jacobian-Free}
    \begin{equation*}
        \alt<2>{\Ib}{\Bb^{-1}} \approx J_{g_{\thetab}}(\zb^\star)^{-1}
    \end{equation*}
\end{frame}

\begin{frame}{Learnlets}
    \begin{figure}
        \centering
        \includegraphics[height=0.8\textheight]{Figures/clinic_applic/learnlets_tikz_reduced.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{Figures/clinic_applic/dsm_main.pdf}
    \end{figure}
\end{frame}
