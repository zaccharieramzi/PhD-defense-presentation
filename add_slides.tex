\appendix

\begin{frame}[noframenumbering, plain]{}
    Backup slides
\end{frame}

% XXX write those
\begin{frame}{MR signal derivation}

\end{frame}

\begin{frame}{Fourier Transform and MRI}

\end{frame}

\begin{frame}{More details on GRAPPA}

\end{frame}

\begin{frame}{Sparsity}

\end{frame}

\begin{frame}{Basis Pursuit}

\end{frame}

\begin{frame}{Sensitivity Maps}

\end{frame}

\begin{frame}{Noise model for MRI}

\end{frame}

\begin{frame}{Proximity operator}

\end{frame}

\begin{frame}{Convolutions}

\end{frame}

\begin{frame}{CNN}
    Convolutional Neural Network~(CNN): chain of Convolution + Nonlinearity.
\end{frame}

\begin{frame}{U-net}
    \begin{figure}
        \centering
        \includegraphics[height=0.6\textheight]{Figures/add_slides/unet_hires.pdf}
        \caption{Illustration from the original paper.\footfullcite{ronneberger2015u}}
    \end{figure}
\end{frame}

\begin{frame}{MWCNN}
    \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{Figures/add_slides/mwcnn.pdf}
        \caption{Illustration from the original paper.\footfullcite{Liu2018}}
    \end{figure}
\end{frame}

\begin{frame}{fastMRI dataset}

\end{frame}

\begin{frame}{2020 fastMRI challenge}
    \begin{itemize}
        \item 2nd edition
        \item 8 participants
        \item Brain data
    \end{itemize}
\end{frame}

\begin{frame}{What did the winners do that I did not}

\end{frame}

\begin{frame}{autoMAP}
    \begin{figure}
        \centering
        \includegraphics[width=0.67\textwidth]{Figures/add_slides/automap.png}
        \caption{Illustration from the original paper.\footfullcite{Zhu2018}}
    \end{figure}
\end{frame}

\begin{frame}{XPDNet - ct'ed}
    \begin{center}
        \scalebox{0.85}{
        \begin{algorithm}[H]
            \SetAlgoLined
            \KwData{$\ybb$ the k-space data, $\Omega$ the Cartesian trajectory, $\Sbb$ the coarse estimates of the sensitivity maps}
            \KwResult{$\xb$, the reconstructed magnitude MR image}
             $\Sbb = g_{\theta_r}(\Sbb)$ ; \tcp{Sensitivity maps refinement}
             Update ${\cal A}$ and ${\cal A}^H$\;
             $\xb =  {\cal A}^H \ybb$\;

             $\xb_b = [\xb, \xb, \xb, \xb, \xb]$; \tcp{Buffer creation, in practice concatenation along the channel dimension}
             \For{$i \leftarrow 1$ \KwTo $N_C$}{
               $\ybb_{res} = {\cal A}\, \xb_b[0] - \ybb $; \tcp{Data consistency}
              $\xb_{dc} = {\cal A}^H \ybb_{res}$; \tcp{Density compensation}
              $\xb_b = \xb_b + f_{\theta_i}([\xb_b, \xb_{dc}]))$; \tcp{Proximity operator learning and nonlinear acceleration scheme}
             }
             $\xb = |\xb_b[0]|$ \tcp{Magnitude computation}
             \caption{\emph{XPDNet}.}
        \end{algorithm}
    }
    \end{center}
\end{frame}

\begin{frame}{PDNet \& Recurent Inference Machines~(RIMs)}
    Bayesian Inverse Problem formulation:
    \begin{equation*}
        \argmax_x p(x|y) \propto p(y|x) p(x)
    \end{equation*}
    Updates:
    \begin{equation*}
        x_{n+1} = x_n + \epsilon_n \nabla_x (\log p(y|x) + \log p(x))(x_n)
    \end{equation*}
    RIMs generalize to:\footfullcite{Putzky2017RecurrentProblems}
    \begin{equation*}
        x_{n+1} = x_n + g(\nabla_x(\log p(y|x))(x_n), x_n)
    \end{equation*}
\end{frame}

\begin{frame}{NC-PDNet - ct'ed}
    \begin{center}
        \scalebox{0.85}{
        \begin{algorithm}[H]
            \SetAlgoLined
            \KwData{$\ybb$ the k-space data, $\Omega$ the non-Cartesian trajectories, $\db$ the pre-computed DCp weights, $\Sbb$ the coarse estimates of the sensitivity maps}
            \KwResult{$\xb$, the reconstructed magnitude MR image}
             $\Sbb = g_{\theta_r}(\Sbb)$ ; \tcp{Sensitivity maps refinement}
             Update ${\cal A}$ and ${\cal A}^H$\;
             $\xb =  {\cal A}^H \ybb$\;

             $\xb_b = [\xb, \xb, \xb, \xb, \xb]$; \tcp{Buffer creation, in practice concatenation along the channel dimension}
             \For{$i \leftarrow 1$ \KwTo $N_C$}{
               $\ybb_{res} = {\cal A}\, \xb_b[0] - \ybb $; \tcp{Data consistency}
              $\xb_{dc} = {\cal A}^H \db\, \ybb_{res}$; \tcp{Density compensation}
              $\xb_b = \xb_b + f_{\theta_i}([\xb_b, \xb_{dc}]))$; \tcp{Proximity operator learning and nonlinear acceleration scheme}
             }
             $\xb = |\xb_b[0]|$ \tcp{Magnitude computation}
             \caption{\emph{NC-PDNet}: Density compensated Primal Dual unrolled neural network over $N_C$ iterations.}
             \label{alg:NC-PDNet}
        \end{algorithm}
    }
    \end{center}
\end{frame}

\begin{frame}{Density Compensation}
    \begin{equation*}
            \db_{n+1} = \frac{\db_n}{{\cal F}_{\Omega} {\cal F}^H_{\Omega} \db_n}
    \end{equation*}
\end{frame}

\begin{frame}{Image quality metrics}
    \begin{multicols}{2}
        \textbf{Peak Signal to Noise Ratio~(PSNR)}:
        \begin{equation*}
            \operatorname{PSNR}(\xb, \hat{\xb}) = 10 \log_{10}\left( \frac{\max_i{\xb_i}}{\frac1n \|\xb - \hat{\xb}\|^2_2}\right)
        \end{equation*}
        \newpage
        \textbf{Structural Similarity Index~(SSIM)}:
        \begin{equation*}
            \operatorname{SSIM}(\xb, \hat{\xb}) = [l(\xb, \hat{\xb})]^\alpha . [c(\xb, \hat{\xb})]^\beta . [s(\xb, \hat{\xb})]^\gamma
        \end{equation*}
        \begin{itemize}
            \item $l$: luminance
            \item $c$: contrast
            \item $s$: structure
        \end{itemize}
    \end{multicols}
    Problem: not a good correlation with the actual visual quality.
\end{frame}

\begin{frame}{quasi-Newton methods - 1}
    Idea: replace the costly Jacobian inverse with a qN matrix $\Bb^{-1}$.\\
    \begin{equation*}
        f(x^\star) = 0
    \end{equation*}
    \begin{multicols}{2}

            \begin{center}
                \textbf{Newton Methods}
                \begin{equation*}
                    x_{n+1} = x_n -\frac{\partial f}{\partial x}(x_n)^{-1}f(x_n)
                \end{equation*}
            \end{center}
            \newpage
            \begin{center}
                \textbf{Quasi-Newton Methods}
                \begin{equation*}
                    x_{n+1} = x_n -\Bb_n^{-1}f(x_n)
                \end{equation*}
            \end{center}
            Update $\Bb_n$ and its inverse with the Sherma-Morrison formula.
    \end{multicols}
\end{frame}

\begin{frame}{quasi-Newton methods - 2}
    Secant conditions: set of conditions $\Bb$ must verify.\\
    Typically: $\Bb_n(x_n - x_{n-1}) = f(x_n) - f(x_{n-1})$.\\
    Multiple solutions $\Rightarrow$ regularization with $\Bb_n = \argmin_{\Bb: \Bb \Delta x_n = \Delta f_n} \|\Bb - \Bb_{n-1}\|$
\end{frame}

\begin{frame}{OPA - 1}
    Outer Problem Awareness: modify the inner problem resolution depending on the outer problem.\\

    Additional updates of $\Bb$ with the OPA direction: $\eb_n = t_n \Bb_n^{-1} \frac{\partial g_{\theta}}{\partial \theta}\Bigr|_{\zb_n}$.
\end{frame}

\begin{frame}{OPA - 2}
    \begin{center}
        \scalebox{.5}{
    \begin{algorithm}[H]
        \SetAlgoRefName{LBFGS}
        \DontPrintSemicolon
        \caption{(Limited memory) BFGS method with OPA.}
        \label{alg:LBFGS}
        \KwIn{ initial guess $(\zb_0, \Bb_0^{-1})$, where $\Bb_0^{-1}$ is symmetric and positive definite, tolerance $\epsilon>0$, frequency of additional updates $M\in\mathbb{N}$, memory limit $L\in\mathbb{N}\cup\{\infty\}$, $(t_n)$ a null sequence of positive numbers with $\sum_n t_n<\infty$	}
        Let $F := \nabla_\zb g_{\theta}$\\
        \For{ $n=0,1,2,\ldots$ }
        {
            \lIf{$\norm{F(\zb_n)}\leq\epsilon$ }{let $\zb^\star:=\zb_n$ and let $\Bb:=\Bb_n$; STOP}
            Let $\hat \Bb_n^{-1}:=\Bb_n^{-1}$\\
            \If{$(n\operatorname{mod} M)=0$}{let $\eb_n:=t_n \Bb_n^{-1} \frac{\partial g_{\theta}}{\partial \theta}\Bigr|_{\zb_n}$,
                $\hat \yb_n:=F(\zb_n+\eb_n)-F(\zb_n)$ and $\hat \rb_n:=(\eb_n)^\top \hat \yb_n$\\
            \If{$\hat \rb_n>0$}{let $\hat \ab_n:=\eb_n - \Bb_n^{-1} \hat \yb_n$ and let
            \begin{equation*}
                \hat \Bb_n^{-1} := \Bb_n^{-1} + \frac{\hat \ab_n (\eb_n)^\top + \eb_n (\hat \ab_n)^\top}{\hat \rb_n} -
                \frac{(\hat \ab_n)^\top \hat \yb_n}{(\hat \rb_n)^2} \eb_n (\eb_n)^\top
            \end{equation*}
            }
        }
            Let $\Bb_n^{-1}:=\hat \Bb_n^{-1}$\\
            \lIf{$n\geq L$}{remove update $n-L$ from $\Bb_n^{-1}$}
            Let $p_n := -\Bb_n^{-1} F(\zb_n)$\\
            Obtain $\alpha_n$ via line-search and let $\sb_n := \alpha_n p_n$\\
            Let $\zb_{n+1}:=\zb_n+\sb_n$, $\yb_n:=F(\zb_{n+1})-F(\zb_n)$ and $\rb_n:=(\sb_n)^\top \yb_n$\\
            \uIf{$\rb_n>0$}{let $\ab_n:=\sb_n-\Bb_n^{-1} \yb_n$ and let
            \begin{equation*}
                \Bb_{n+1}^{-1} := \Bb_n^{-1} + \frac{\ab_n (\sb_n)^\top + \sb_n (\ab_n)^\top}{\rb_n} -
                \frac{(\ab_n)^\top \yb_n}{(\rb_n)^2} \sb_n (\sb_n)^\top
            \end{equation*}
            }
            \lElse{let $\Bb_{n+1}^{-1}:=\Bb_n^{-1}$}
            \lIf{$n\geq L$}{remove update $n-L$ from $\Bb_{n+1}^{-1}$}
        }
        \KwOut{$\zb^\star$, $\Bb$}
    \end{algorithm}
    }
    \end{center}
\end{frame}

\begin{frame}{Adjoint Broyden}
    For DEQs, vanilla OPA direction involves $\frac{\partial g_{\thetab}}{\partial \thetab}\Bigr|_{\zb_n}$ $\Rightarrow$ inefficient.\\

    Recall that the SHINE direction is: $\pb_\thetab =  \highlight{blue}{$\nabla_\zb \mathcal{L}(\zb^\star)$} B^{-1} \frac{\partial g_{\thetab}}{\partial \thetab}\Bigr|_{\zb^\star}$.\\
    Other direction, \highlight{blue}{$\nabla_\zb \mathcal{L}(\zb^\star)$}, in left-multiplication $\Rightarrow$ Adjoint Broyden for the secant condition to work.
\end{frame}

\begin{frame}{Jacobian-Free}
    \begin{equation*}
        \alt<2>{\Ib}{\Bb^{-1}} \approx J_{g_{\thetab}}(\zb^\star)^{-1}
    \end{equation*}
\end{frame}